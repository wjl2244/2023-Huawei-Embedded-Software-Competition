{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import model\n",
    "import imutils\n",
    "from torchvision import transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.778885630498534\n",
      "电量：Battery_Medium\n"
     ]
    }
   ],
   "source": [
    "# 创建机器人\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "robot = Robot()\n",
    "# 设置灯光\n",
    "from RGB_Lib import Programing_RGB\n",
    "RGB = Programing_RGB()\n",
    "RGB.OFF_ALL_RGB()  # 关闭所有灯光，如果出现流水灯则表示需要重新摆放位置\n",
    "\n",
    "# 显示电量\n",
    "from Battery_Vol_Lib import BatteryLevel\n",
    "batteryLevel = BatteryLevel()\n",
    "print(f\"电量：{batteryLevel.Update()}\")\n",
    "\n",
    "# 创建相机实例\n",
    "camera = Camera.instance(width=720, height=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_net():\n",
    "    def __init__(self):\n",
    "        model_path = '/home/jetbot/car_train/image_code/workdir/Res18_ep200_lr1.0E-03_bs64/model_para.pth'\n",
    "\n",
    "        self.device = device\n",
    "        self.net = model.resnet18(n_classes=4, input_channels=4).to(device)\n",
    "        print(device)\n",
    "        self.net.load_state_dict(torch.load(model_path))\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def get_object_position(self, fig):\n",
    "        fig = self.preprocess(fig)        \n",
    "        fig = self.transform(fig).to(device)\n",
    "        fig = fig.unsqueeze(0)\n",
    "        begin_time = time.time()\n",
    "        pred = self.net(fig)\n",
    "        end_time = time.time()\n",
    "        # print('net time:{}s'.format((end_time-begin_time)/1000))        \n",
    "        pred[:,2] = (pred[:,2] - 0.5) * 2\n",
    "        pred[:,3] = pred[:,3] * 2\n",
    "        print('pred:',pred[:,2:])\n",
    "        return pred[:,2:]\n",
    "    def preprocess(self, img):\n",
    "        greenLower = (29, 86, 6)\n",
    "        greenUpper = (64, 255, 255)        \n",
    "        width, height = img.shape[:2]\n",
    "        blurred = cv2.GaussianBlur(img, (11, 11), 0)        \n",
    "        hsv = cv2.cvtColor(blurred, cv2.COLOR_RGB2HSV)\n",
    "        # cv2.imwrite('./hsv.png', hsv)        \n",
    "        mask = cv2.inRange(hsv, greenLower, greenUpper)\n",
    "        mask = cv2.erode(mask, None, iterations=2)\n",
    "        mask = cv2.dilate(mask, None, iterations=2)\n",
    "        # cv2.imwrite('./mask.png', mask)        \n",
    "        \n",
    "        cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "                                cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        center = None\n",
    "\n",
    "\n",
    "        ret_mask = np.zeros_like(mask)\n",
    "        if len(cnts) > 0:\n",
    "            c = max(cnts, key=cv2.contourArea)\n",
    "            ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "            M = cv2.moments(c)\n",
    "            center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "\n",
    "            # To see the centroid clearly\n",
    "            if radius > 10:\n",
    "                # cv2.circle(img, (int(x), int(y)), int(radius), (0, 255, 255), 5)\n",
    "                # cv2.imwrite(\"circled_frame.png\", cv2.resize(img, (int(height / 2), int(width / 2))))\n",
    "                # cv2.circle(img, center, 5, (0, 0, 255), -1)\n",
    "                cv2.circle(ret_mask, (int(x), int(y)), int(radius), 255, -1)\n",
    "\n",
    "        # cv2.imwrite('ret_mask.png', ret_mask)\n",
    "        # cv2.imwrite('./preprocess.png', img)           \n",
    "\n",
    "        mask_img = np.concatenate((img, ret_mask[:,:,None]), axis=2)\n",
    "        return mask_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self):\n",
    "        self.dim_state = 2\n",
    "        # 动作维度，左右轮速度\n",
    "        self.dim_action = 2\n",
    "        self.action_min = -1.2\n",
    "        self.action_max = 1.2\n",
    "\n",
    "    def get_image(self):\n",
    "        fig = camera.value\n",
    "        sensorImage = cv2.resize(fig, (720,720))  # 因为cv2版本问题可能会报错\n",
    "        return sensorImage\n",
    "\n",
    "    def close(self):\n",
    "        sim.simxStopSimulation(self.clientId, sim.simx_opmode_blocking)\n",
    "\n",
    "    def step(self, action, i, img_net):\n",
    "#         time6 = time.time()\n",
    "#         object_position = img_net.get_object_position(self.get_image())\n",
    "#         time7 = time.time()\n",
    "#         print('image_net_time:', (time7-time6)/1000)\n",
    "        object_position = [[1,1]]\n",
    "        robot.set_motors(float(action[0,0]), float(action[0,1]))\n",
    "        time.sleep(1.0)\n",
    "        robot.stop()\n",
    "        print('object_position:', object_position)\n",
    "        dis_x = object_position[0][0]\n",
    "        dis_z = object_position[0][1] - 0.6\n",
    "        reward = -(dis_x ** 2 + dis_z ** 2 + 0.01 * i)\n",
    "        if abs(dis_x) < 0.1 and abs(dis_z) < 0.1:\n",
    "            done = True\n",
    "            reward = 0\n",
    "        else:\n",
    "            done = False\n",
    "        next_state = object_position\n",
    "        return next_state, reward, done\n",
    "    \n",
    "class ReplayBeffer():\n",
    "    def __init__(self, buffer_maxlen):\n",
    "        self.buffer = deque(maxlen=buffer_maxlen)\n",
    "\n",
    "    def push(self, data):\n",
    "        self.buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        reward_list = []\n",
    "        next_state_list = []\n",
    "        done_list = []\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        for experience in batch:\n",
    "            s, a, r, n_s, d = experience\n",
    "            # state, action, reward, next_state, done\n",
    "\n",
    "            state_list.append(s)\n",
    "            action_list.append(a)\n",
    "            reward_list.append(r)\n",
    "            next_state_list.append(n_s)\n",
    "            done_list.append(d)\n",
    "\n",
    "            \n",
    "\n",
    "        return torch.FloatTensor(state_list).to(device), \\\n",
    "               torch.FloatTensor(action_list).to(device), \\\n",
    "               torch.FloatTensor(reward_list).unsqueeze(-1).to(device), \\\n",
    "               torch.FloatTensor(next_state_list).to(device), \\\n",
    "               torch.FloatTensor(done_list).unsqueeze(-1).to(device)\n",
    "\n",
    "    def buffer_len(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# Value Net\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, state_dim, edge=3e-3):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim, 256)\n",
    "        self.linear2 = nn.Linear(256, 256)\n",
    "        self.linear3 = nn.Linear(256, 2)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-edge, edge)\n",
    "        self.linear3.bias.data.uniform_(-edge, edge)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Soft Q Net\n",
    "class SoftQNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, edge=3e-3):\n",
    "        super(SoftQNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.linear2 = nn.Linear(256, 256)\n",
    "        self.linear3 = nn.Linear(256, 2)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-edge, edge)\n",
    "        self.linear3.bias.data.uniform_(-edge, edge)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Policy Net\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, log_std_min=-20, log_std_max=2, edge=3e-3):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "        self.linear1 = nn.Linear(state_dim, 256)\n",
    "        self.linear2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.mean_linear = nn.Linear(256, action_dim)\n",
    "        self.mean_linear.weight.data.uniform_(-edge, edge)\n",
    "        self.mean_linear.bias.data.uniform_(-edge, edge)\n",
    "\n",
    "        self.log_std_linear = nn.Linear(256, action_dim)\n",
    "        self.log_std_linear.weight.data.uniform_(-edge, edge)\n",
    "        self.log_std_linear.bias.data.uniform_(-edge, edge)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def action(self, state):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "\n",
    "        z = normal.sample()\n",
    "        action = torch.tanh(z).detach().cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    # Use re-parameterization tick\n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        noise = Normal(0, 1)\n",
    "\n",
    "        z = noise.sample()\n",
    "        action = torch.tanh(mean + std * z.to(device))\n",
    "        log_prob = normal.log_prob(mean + std * z.to(device)) - torch.log(1 - action.pow(2) + epsilon)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "class SAC:\n",
    "    def __init__(self, env, gamma, tau, buffer_maxlen, value_lr, q_lr, policy_lr):\n",
    "\n",
    "        self.env = env\n",
    "        self.state_dim = env.dim_state\n",
    "        self.action_dim = env.dim_action\n",
    "\n",
    "        # hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize networks\n",
    "        self.value_net = ValueNet(self.state_dim).to(device)\n",
    "        self.target_value_net = ValueNet(self.state_dim).to(device)\n",
    "        self.q1_net = SoftQNet(self.state_dim, self.action_dim).to(device)\n",
    "        self.q2_net = SoftQNet(self.state_dim, self.action_dim).to(device)\n",
    "        self.policy_net = PolicyNet(self.state_dim, self.action_dim).to(device)\n",
    "\n",
    "        # Load the target value network parameters\n",
    "        for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=value_lr)\n",
    "        self.q1_optimizer = optim.Adam(self.q1_net.parameters(), lr=q_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2_net.parameters(), lr=q_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "        # Initialize thebuffer\n",
    "        self.buffer = ReplayBeffer(buffer_maxlen)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action = self.policy_net.action(state)\n",
    "        return action\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        state, action, reward, next_state, done = self.buffer.sample(batch_size)\n",
    "        new_action, log_prob = self.policy_net.evaluate(state)\n",
    "\n",
    "        # V value loss\n",
    "        value = self.value_net(state)\n",
    "        new_q1_value = self.q1_net(state, new_action)\n",
    "        new_q2_value = self.q2_net(state, new_action)\n",
    "        next_value = torch.min(new_q1_value, new_q2_value) - log_prob\n",
    "        value_loss = F.mse_loss(value, next_value.detach())\n",
    "\n",
    "        # Soft q  loss\n",
    "        q1_value = self.q1_net(state, action)\n",
    "        q2_value = self.q2_net(state, action)\n",
    "        target_value = self.target_value_net(next_state)\n",
    "        target_q_value = reward + done * self.gamma * target_value\n",
    "        q1_value_loss = F.mse_loss(q1_value, target_q_value.detach())\n",
    "        q2_value_loss = F.mse_loss(q2_value, target_q_value.detach())\n",
    "\n",
    "        # Policy loss\n",
    "        policy_loss = (log_prob - torch.min(new_q1_value, new_q2_value)).mean()\n",
    "\n",
    "        # Update Policy\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Update v\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        # Update Soft q\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q1_value_loss.backward()\n",
    "        q2_value_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "        self.q2_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        for target_param, param in zip(self.target_value_net.parameters(), self.value_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(env, agent, Episode, batch_size):\n",
    "    Return = []\n",
    "    action_range = [env.action_min, env.action_max]\n",
    "    img_net = image_net()\n",
    "\n",
    "    for episode in range(Episode):\n",
    "        score = 0\n",
    "        state = img_net.get_object_position(env.get_image())  # 重新摆放之后获取目标位置\n",
    "#         state = [[1,1]]\n",
    "        for i in range(300):\n",
    "            time1 = time.time()\n",
    "            action = agent.get_action(state)\n",
    "            # action output range[-1,1],expand to allowable range\n",
    "            action_in = action * (action_range[1] - action_range[0]) + (action_range[1] + action_range[0])\n",
    "            next_state, reward, done= env.step(action_in, i, img_net)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            agent.buffer.push((state, action, reward, next_state, done_mask))\n",
    "            state = next_state\n",
    "\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            if agent.buffer.buffer_len() > 500:\n",
    "                agent.update(batch_size)\n",
    "\n",
    "        print(\"episode:{}, Return:{}, buffer_capacity:{}\".format(episode, score, agent.buffer.buffer_len()))\n",
    "        Return.append(score)\n",
    "        RGB.Set_WaterfallLight_RGB()  # 设置灯光\n",
    "        time.sleep(5.0)  # 等待五秒，重新摆放位置，开始下一轮探索\n",
    "        RGB.OFF_ALL_RGB()\n",
    "        if  batteryLevel.Update() == \"Battery_Low\":\n",
    "            RGB.Set_All_RGB(0xFF, 0x00, 0x00)  # 红灯亮起\n",
    "            print(\"低电量！\")\n",
    "    env.close()\n",
    "    plt.plot(Return)\n",
    "    plt.ylabel('Return')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device('cuda')\n",
    "    device = \"cpu\"\n",
    "    # Params\n",
    "    tau = 0.01\n",
    "    gamma = 0.99\n",
    "    q_lr = 3e-3\n",
    "    value_lr = 3e-3\n",
    "    policy_lr = 3e-3\n",
    "    buffer_maxlen = 50000\n",
    "\n",
    "    Episode = 100\n",
    "    batch_size = 128\n",
    "    quit()\n",
    "    agent = SAC(env, gamma, tau, buffer_maxlen, value_lr, q_lr, policy_lr)\n",
    "    main(env, agent, Episode, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
